{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "from pyspark.rdd import *\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local\",\"music\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sampleUsersPath = \"sampleUsers.txt\"\n",
    "sampleTracksPath = \"sampleTracks.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate random data from sample\n",
    "import random\n",
    "\n",
    "randomUsersPath = \"randomusers.txt\"\n",
    "\n",
    "with open(sampleUsersPath) as sampleFile:\n",
    "    lines = random.sample(sampleFile.readlines(), 2000)\n",
    "\n",
    "outF = open(randomUsersPath, \"w\")\n",
    "outF.writelines(lines)\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "Load data as Spark's dataframe.  \n",
    "Columns separated by \"\\t\".  \n",
    "Counts higher than 20 are reduced to 20.  \n",
    "\n",
    "Name|Type\n",
    "-------:|-----\n",
    "user| StringType()|\n",
    "song| StringType()|\n",
    "count| IntegerType()|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    csvSchema = (StructType() \n",
    "                .add(\"user\",StringType())\n",
    "                .add(\"song\",StringType())\n",
    "                .add(\"count\",IntegerType()))\n",
    "   \n",
    "    df = spark.read.load(path,format=\"csv\",sep = \"\\t\", schema = csvSchema)\n",
    "\n",
    "    newdf = df.withColumn(\"count\",\n",
    "                          pyspark.sql.functions.when(\n",
    "                              pyspark.sql.functions.col(\"count\") > 20,20     #if row with count > 20 True, replace with 20\n",
    "                          ).otherwise(\n",
    "                      pyspark.sql.functions.col(\"count\")                     #otherwise replace with default count\n",
    "                          )\n",
    "                         )\n",
    "\n",
    "    \n",
    "    #newdf[newdf['count'] > 20].show(5) \n",
    "    #newdf.show(5) \n",
    "    #print(df[df['count'] > 20].show(5))\n",
    "    return newdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|                user|              song|count|\n",
      "+--------------------+------------------+-----+\n",
      "|b80344d063b5ccb32...|SOBBMDR12A8C13253B|    2|\n",
      "|b80344d063b5ccb32...|SODZWFT12A8C13C0E4|    1|\n",
      "|b80344d063b5ccb32...|SOHQWYZ12A6D4FA701|    1|\n",
      "|b80344d063b5ccb32...|SOJNNUA12A8AE48C7A|    1|\n",
      "|b80344d063b5ccb32...|SOLXHAI12A6D4FD6F3|    1|\n",
      "|b80344d063b5ccb32...|SOOSIVQ12A6D4F8AE0|    1|\n",
      "|b80344d063b5ccb32...|SORJNVW12A8C13BF90|    1|\n",
      "|85c1f87fea955d09b...|SODJTHN12AF72A8FCD|    2|\n",
      "|85c1f87fea955d09b...|SOIDFHN12A8C13ABAC|    2|\n",
      "|4bd88bfb25263a75b...|SOWEHOM12A6BD4E09E|    1|\n",
      "|9d6f0ead607ac2a6c...|SOCLQES12A58A7BB1D|    2|\n",
      "|9d6f0ead607ac2a6c...|SOKLRPJ12A8C13C3FE|    2|\n",
      "|9bb911319fbc04f01...|SOXBXBI12A8C13C71C|    5|\n",
      "|b64cdd1a0bd907e5e...|SOBDWET12A6701F114|    2|\n",
      "|b64cdd1a0bd907e5e...|SOLQYOG12B0B80BA71|    2|\n",
      "|b64cdd1a0bd907e5e...|SOZPQES12A6D4F8E57|    2|\n",
      "|17aa9f6dbdf753831...|SODHHEG12A58A779FB|    2|\n",
      "|17aa9f6dbdf753831...|SODUANR12A6D4F5036|    1|\n",
      "|17aa9f6dbdf753831...|SOJPFPR12AB018109D|    1|\n",
      "|17aa9f6dbdf753831...|SOJUVYA12AB01860BA|    2|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded = load(sampleUsersPath).persist()\n",
    "loaded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load test'''\n",
    "correctCols = StructType([\\\n",
    "StructField(\"user\",StringType(),True),\\\n",
    "StructField(\"song\",StringType(),True),\\\n",
    "StructField(\"count\",IntegerType(),True)])\n",
    "\n",
    "fakeData = [(\"abc123\", \"123abc\", 2)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, correctCols)\n",
    "\n",
    "assert loaded.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, loaded.dtypes)\n",
    "\n",
    "assert loaded.filter('count>20').count() == 0, \"counts higher than 20 was expected to be 0 but it was %s\" % loaded.filter('count>20').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert\n",
    "\n",
    "Convert user and song fields into doubles. Use mllib's StringIndexer. \n",
    "\n",
    "Name|Type\n",
    "-------:|-----\n",
    "user| StringType()|\n",
    "song| StringType()|\n",
    "count| IntegerType()|\n",
    "user_indexed |DoubleType()|\n",
    "song_indexed |DoubleType()|\n",
    "\n",
    "param `df` Dataframe which has been created using method `load()`  \n",
    "`return` Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(df):\n",
    "    ColInputs = [\"user\",\"song\"]\n",
    "    ColOutputs = [\"user_indexed\",\"song_indexed\"]\n",
    "    stringIndexer = StringIndexer(inputCols=ColInputs,outputCols=ColOutputs)\n",
    "    model = stringIndexer.fit(df)\n",
    "    newdf = model.transform(df)\n",
    "    \n",
    "    #sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),key=lambda x: x[0])\n",
    "    return newdf\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------------+------------+\n",
      "|                user|              song|count|user_indexed|song_indexed|\n",
      "+--------------------+------------------+-----+------------+------------+\n",
      "|b80344d063b5ccb32...|SOBBMDR12A8C13253B|    2|       162.0|       577.0|\n",
      "|b80344d063b5ccb32...|SODZWFT12A8C13C0E4|    1|       162.0|      1053.0|\n",
      "|b80344d063b5ccb32...|SOHQWYZ12A6D4FA701|    1|       162.0|      1646.0|\n",
      "|b80344d063b5ccb32...|SOJNNUA12A8AE48C7A|    1|       162.0|      1945.0|\n",
      "|b80344d063b5ccb32...|SOLXHAI12A6D4FD6F3|    1|       162.0|      2306.0|\n",
      "|b80344d063b5ccb32...|SOOSIVQ12A6D4F8AE0|    1|       162.0|      2702.0|\n",
      "|b80344d063b5ccb32...|SORJNVW12A8C13BF90|    1|       162.0|      3124.0|\n",
      "|85c1f87fea955d09b...|SODJTHN12AF72A8FCD|    2|       810.0|       951.0|\n",
      "|85c1f87fea955d09b...|SOIDFHN12A8C13ABAC|    2|       810.0|      1728.0|\n",
      "|4bd88bfb25263a75b...|SOWEHOM12A6BD4E09E|    1|      1151.0|      3824.0|\n",
      "|9d6f0ead607ac2a6c...|SOCLQES12A58A7BB1D|    2|       839.0|       803.0|\n",
      "|9d6f0ead607ac2a6c...|SOKLRPJ12A8C13C3FE|    2|       839.0|         5.0|\n",
      "|9bb911319fbc04f01...|SOXBXBI12A8C13C71C|    5|      1317.0|      3948.0|\n",
      "|b64cdd1a0bd907e5e...|SOBDWET12A6701F114|    2|       560.0|       586.0|\n",
      "|b64cdd1a0bd907e5e...|SOLQYOG12B0B80BA71|    2|       560.0|       245.0|\n",
      "|b64cdd1a0bd907e5e...|SOZPQES12A6D4F8E57|    2|       560.0|      4289.0|\n",
      "|17aa9f6dbdf753831...|SODHHEG12A58A779FB|    2|       115.0|       934.0|\n",
      "|17aa9f6dbdf753831...|SODUANR12A6D4F5036|    1|       115.0|      1013.0|\n",
      "|17aa9f6dbdf753831...|SOJPFPR12AB018109D|    1|       115.0|      1958.0|\n",
      "|17aa9f6dbdf753831...|SOJUVYA12AB01860BA|    2|       115.0|      1988.0|\n",
      "+--------------------+------------------+-----+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted = convert(loaded).persist()\n",
    "converted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''convert test'''\n",
    "correctCols = StructType([\\\n",
    "StructField(\"user\",StringType(),True),\\\n",
    "StructField(\"song\",StringType(),True),\\\n",
    "StructField(\"count\",IntegerType(),True),\\\n",
    "StructField(\"user_indexed\",DoubleType(),True),\\\n",
    "StructField(\"song_indexed\",DoubleType(),True)])\n",
    "\n",
    "fakeData = [(\"abc123\", \"123abc\", 2, 1.0, 2.0)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, correctCols)\n",
    "\n",
    "assert converted.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, converted.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Rating\n",
    "\n",
    "create RDD of Rating classes.  \n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.Rating\n",
    "\n",
    "The params of the Rating function (user=`user_indexed`, product=`song_indexed`and rating=`count`) \n",
    "\n",
    "param `df` Dataframe which has `user_indexed` and `song_indexed` fields (output from `convert()` method)  \n",
    "`return` RDD containg Rating objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toRating(df):\n",
    "    rowRDD = df.rdd\n",
    "    #print(rowRDD.map(lambda x: x[\"user\"]).take(4))\n",
    "    map1 = rowRDD.map(lambda x: Rating(user=x[\"user_indexed\"],product=x[\"song_indexed\"],rating=x[\"count\"]))\n",
    "    #print(map1.take(4))\n",
    "    #r = Rating\n",
    "    \n",
    "    return map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=162, product=577, rating=2.0),\n",
       " Rating(user=162, product=1053, rating=1.0),\n",
       " Rating(user=162, product=1646, rating=1.0),\n",
       " Rating(user=162, product=1945, rating=1.0),\n",
       " Rating(user=162, product=2306, rating=1.0),\n",
       " Rating(user=162, product=2702, rating=1.0),\n",
       " Rating(user=162, product=3124, rating=1.0),\n",
       " Rating(user=810, product=951, rating=2.0),\n",
       " Rating(user=810, product=1728, rating=2.0),\n",
       " Rating(user=1151, product=3824, rating=1.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rated = toRating(converted).persist()\n",
    "rated.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''toRating tests'''\n",
    "rows = [Rating(user=162, product=577, rating=2.0),\n",
    " Rating(user=162, product=1053, rating=1.0),\n",
    " Rating(user=162, product=1646, rating=1.0),\n",
    " Rating(user=162, product=1945, rating=1.0),\n",
    " Rating(user=162, product=2306, rating=1.0)]\n",
    "assert rated.take(5) == rows, \"the first 5 rows were expected to be %s but they were %s\" % (rows, rated.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ALS\n",
    "\n",
    "train ALS model. \n",
    "https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#collaborative-filtering\n",
    "\n",
    "param `data` RDD of Rating objects that is created using `toRating()` method.  \n",
    "param `seed` value used for testing purposes. \n",
    "`return` trained ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainALS(data, seed):\n",
    "    model = ALS.train(data,rank=2,seed=seed)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rSeed = random.randint(0, 10000)\n",
    "model = trainALS(rated, rSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend Songs\n",
    "\n",
    "Recommend 5 songs to a given user.\n",
    "\n",
    "param `model` trained ALS model  \n",
    "param `user` user id converted (user_indexed) to Integer (with `convert()`)  \n",
    "`return` recommendations in Array  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendSongs(model, user):\n",
    "    prediction = model.recommendProducts(user,5)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=162, product=391, rating=20.684566349984323),\n",
       " Rating(user=162, product=1334, rating=19.216739435985964),\n",
       " Rating(user=162, product=3283, rating=18.964715987473852),\n",
       " Rating(user=162, product=157, rating=18.746516460716293),\n",
       " Rating(user=162, product=3674, rating=18.70421335719181)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommends = recommendSongs(model, 162)\n",
    "recommends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model and recommendSongs tests'''\n",
    "assert type(recommends[0]) == pyspark.mllib.recommendation.Rating, \"the type was expected to be pyspark.mllib.recommendation.Rating  but it was %s\" % type(recommends[0]) \n",
    "assert recommends[0].user == 162, \"the user was expected to be 162 but it was %s\" % recommends[0].user\n",
    "assert len(recommends) == 5, \"the amount of recommendations was expected to be 5 but it was %s\" % len(recommends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Song Names\n",
    "\n",
    "Input: List RatingObject  \n",
    "Output: List (List SongName Artist)  \n",
    "Map each RatingObject to a Pair of  SongName and Artist  \n",
    "\n",
    "\n",
    "Example\n",
    "Input:\n",
    "\n",
    "`[Rating(182412,218057,29.471691093542958),  \n",
    "Rating(182412,206693,28.39630617887081),  \n",
    "Rating(182412,230654,28.090021579109706),  \n",
    "Rating(182412,200542,27.43900484648816),  \n",
    "Rating(182412,254771,24.82362529344695)] ` \n",
    "\n",
    "Output:\n",
    "\n",
    "`[[\"My Business\",\"Guy\"],  \n",
    "[\"The Man With The Bag\",\"Floyd/Animal/Zoot\"],  \n",
    "[\"Challenger\",\"Growing\"],  \n",
    "[\"Una Ragazza In Due\", \"I Giganti\"],\n",
    "[\"That Is Why\", \"Say Anything\"]]`  \n",
    "\n",
    "Return \\[\\[SongName,NameOfBand\\]...\\]\n",
    "\n",
    "Convert unique_tracks into dataframe, Cols seperated by `<SEP>`. The schema is shown b elow\n",
    "\n",
    "Name|Type\n",
    "-------:|-----\n",
    "track_id| StringType()|\n",
    "song_id| StringType()|\n",
    "artist| StringType()|\n",
    "title| StringType()|\n",
    "\n",
    "1. Filter `converted` dataframe based on if the `song_indexed` value is found in the Rating object list.  \n",
    "Join `converted` with `song_indexed` on \"title\" and  \"artist\" columns.\n",
    "2. Remove duplicates  \n",
    "3. convert dataframe into rdd using collect()  \n",
    "\n",
    "\n",
    "param `converted` Dataframe created using `convert()` method  \n",
    "param `ar` Array of Rating object produced using `recommendSongs()`  \n",
    "param `path` path to unique track names file  \n",
    "`return` corresponding song + artist names inside array, e.g., [['Our Song', 'Taylor Swift'],\n",
    " ['Boom (2006 Remastered Album Version)', 'P.O.D.']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSongNames(converted, ar, path):\n",
    "    RecommendedSongIndexList = [i.product for i in ar]\n",
    "    #print(RecommendedSongIndexList)\n",
    "    csvSchema = (StructType() \n",
    "                .add(\"track_id\",StringType())\n",
    "                .add(\"song_id\",StringType())\n",
    "                .add(\"artist\",StringType())\n",
    "                .add(\"title\",StringType())\n",
    "                )\n",
    "    df = spark.read.load(path,format=\"csv\",sep = \"<SEP>\", schema = csvSchema)\n",
    "    \n",
    "    #df.show(5)\n",
    "    #converted.show(5)\n",
    "    \n",
    "    filteredDF = converted[converted[\"song_indexed\"].isin(RecommendedSongIndexList)]\n",
    "    joinedDF = filteredDF.join(df,filteredDF.song == df.song_id,\"INNER\").drop(df.song_id)\n",
    "    outDF = joinedDF[[\"title\",\"artist\"]].drop_duplicates()\n",
    "    #filteredDF.show(3)\n",
    "    \n",
    "    outRDD = outDF.rdd.map(list)\n",
    "    return outRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"The Emperor's New Clothes\", \"Sinéad O'Connor\"],\n",
       " ['Alhos Verdes', 'GNR'],\n",
       " ['Whataya Want From Me', 'Adam Lambert'],\n",
       " ['Street Justice', 'MSTRKRFT'],\n",
       " ['North Sea Storm (Live)', 'Amon Amarth']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songNames = getSongNames(converted, recommends, sampleTracksPath)\n",
    "songNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''getSongNames test'''\n",
    "assert len(songNames) == 5, \"the amount of song names was expected to be 5 but it was %s\" % len(songNames)\n",
    "assert type(songNames[0]) == list, \"the type of a songNames element was expected to be list but it was %s\" % type(songNames[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend\n",
    "Input: user_id\n",
    "Output: List (List SongName Artist) representing recommendations\n",
    "\n",
    "Example return\n",
    "\n",
    "`[[\"My Business\",\"Guy\"],  \n",
    "[\"The Man With The Bag\",\"Floyd/Animal/Zoot\"],  \n",
    "[\"Challenger\",\"Growing\"],  \n",
    "[\"Una Ragazza In Due\", \"I Giganti\"],\n",
    "[\"That Is Why\", \"Say Anything\"]]`   \n",
    "\n",
    "param `path` path to user data  \n",
    "param `userId` user_id (String) that can be found from user dataset  \n",
    "param `tracksPath` path to unique song names dataset   \n",
    "param `seed` used for testing, put it into the `trainsALS()` method   \n",
    "`return` corresponding song + artist names inside array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(path, userId, tracksPath, seed):\n",
    "    \n",
    "    def getUserIndex(uId,conversionTable):\n",
    "        cDF = conversionTable.where(conversionTable.user == uId).select(conversionTable.user_indexed)\n",
    "        #.where is same as .select\n",
    "        return int(cDF.rdd.first()[\"user_indexed\"])\n",
    "            \n",
    "    loaded = load(path).persist()\n",
    "    converted = convert(loaded).persist()\n",
    "\n",
    "    ConvertedUserId = getUserIndex(userId,converted)\n",
    "    \n",
    "    rated = toRating(converted).persist()\n",
    "    model = trainALS(rated,seed)\n",
    "    recommends = recommendSongs(model,ConvertedUserId)\n",
    "    songNames = getSongNames(converted,recommends,tracksPath)\n",
    "    \n",
    "    return songNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"The Emperor's New Clothes\", \"Sinéad O'Connor\"],\n",
       " ['Alhos Verdes', 'GNR'],\n",
       " ['Whataya Want From Me', 'Adam Lambert'],\n",
       " ['Street Justice', 'MSTRKRFT'],\n",
       " ['North Sea Storm (Live)', 'Amon Amarth']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recom = recommend(sampleUsersPath, \"b80344d063b5ccb3212f76538f3d9e43d87dca9e\" ,sampleTracksPath, rSeed)\n",
    "recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''recommend test'''\n",
    "assert len(recom) == 5, \"the amount of recommendations was expected to be 5 but it was %s\" % len(recom)\n",
    "assert type(recom[0]) == list, \"the type of a 'recommend' element was expected to be list but it was %s\" % type(recom[0])\n",
    "#test if the same user and seed returns the same as songNames\n",
    "assert recom == songNames, \"the song names were expected to be %s but they were %s\" % (songNames, recom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pDL] *",
   "language": "python",
   "name": "conda-env-.conda-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
