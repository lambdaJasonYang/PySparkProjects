{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"main\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .getOrCreate()\\\n",
    "\n",
    "#names of tables\n",
    "airTraffic = \"airtraffic\"\n",
    "carriers = \"carriers\"\n",
    "airports = \"airports\"\n",
    "\n",
    "carriersTable = spark.read.csv(\"carriers.csv\", inferSchema=\"true\", header=\"true\")\n",
    "carriersTable.createOrReplaceTempView(carriers)\n",
    "\n",
    "airportsTable = spark.read.csv(\"airports.csv\", inferSchema=\"true\", header=\"true\")\n",
    "airportsTable.createOrReplaceTempView(airports)\n",
    "\n",
    "sampleFile = \"2008_sample.csv\"\n",
    "testFile = \"2008_testsample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if arrays that contain Row are equal\n",
    "def correctRows(testArray, correctArray):\n",
    "    for i in range(0, len(correctArray)):\n",
    "        assert testArray[i].asDict() == correctArray[i].asDict(), \"the row was expected to be %s but it was %s\" % (correctArray[i].asDict(), testArray[i].asDict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Bind\n",
    "Schema for airTraffic  \n",
    "\n",
    "Name | Type\n",
    "------| :-----\n",
    "Year  | integer (nullable = true)\n",
    "Month | integer (nullable = true)\n",
    "DayofMonth | integer (nullable = true)\n",
    "DayOfWeek | integer (nullable = true)\n",
    "DepTime | integer (nullable = true)\n",
    "CRSDepTime | integer (nullable = true)\n",
    "ArrTime | integer (nullable = true)\n",
    "CRSArrTime | integer (nullable = true)\n",
    "UniqueCarrier | string (nullable = true)\n",
    "FlightNum | integer (nullable = true)\n",
    "TailNum | string (nullable = true)\n",
    "ActualElapsedTime | integer (nullable = true)\n",
    "CRSElapsedTime | integer (nullable = true)\n",
    "AirTime | integer (nullable = true)\n",
    "ArrDelay | integer (nullable = true)\n",
    "DepDelay | integer (nullable = true)\n",
    "Origin | string (nullable = true)\n",
    "Dest | string (nullable = true)\n",
    "Distance | integer (nullable = true)\n",
    "TaxiIn | integer (nullable = true)\n",
    "TaxiOut | integer (nullable = true)\n",
    "Cancelled | integer (nullable = true)\n",
    "CancellationCode | string (nullable = true)\n",
    "Diverted | integer (nullable = true)\n",
    "CarrierDelay | integer (nullable = true)\n",
    "WeatherDelay | integer (nullable = true)\n",
    "NASDelay | integer (nullable = true)\n",
    "SecurityDelay | integer (nullable = true)\n",
    "LateAircraftDelay | integer (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBind(path,name):\n",
    "    #load the raw dataSet to Spark dataframe\n",
    "    df = spark.read.csv(path,nullValue=\"NA\",inferSchema=\"true\", header=\"true\") \n",
    "    \n",
    "    #bind the Spark dataframe to a name, which can be referred to in SQL\n",
    "    #Notice gotcha, it's the Spark dataframe that binds the name\n",
    "    df.createOrReplaceTempView(name) \n",
    "    \n",
    "    return df\n",
    "loadDataAndRegister = lambda x: loadBind(x,airTraffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    2|        28|        4|   1806|      1805|   1818|      1820|           WN|     1642| N392SW|               72|            75|     55|      -2|       1|   SLC| LAS|     368|     6|     11|        1|               D|       0|        null|        null|    null|         null|             null|\n",
      "|2008|    4|         6|        7|   1527|      1531|   1636|      1627|           NW|     1757|  N9337|               69|            56|     30|       9|      -4|   DTW| CMH|     155|     2|     37|        0|            null|       0|        null|        null|    null|         null|             null|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "schema\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "data.show(2) \n",
    "print(\"schema\")\n",
    "#carrierData = loadBind(sampleFile,carriers)\n",
    "#print(carrierData.schema)\n",
    "#spark.read.table(airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loadDataAndRegister tests'''\n",
    "\n",
    "df = loadBind(testFile,airTraffic)\n",
    "\n",
    "# Table \"airtraffic\" should exists\n",
    "assert spark.sql(\"SHOW TABLES Like 'airtraffic'\").count() == 1, \"there was expected to be a table called 'airtraffic'\"\n",
    "\n",
    "# Columns should have correct values\n",
    "third = df.collect()[2]\n",
    "correctRow = Row(Year=2008, Month=5, DayofMonth=6, DayOfWeek=2, DepTime=611,\n",
    "                             CRSDepTime=615, ArrTime=729, CRSArrTime=735, UniqueCarrier='EV',\n",
    "                             FlightNum=4794, TailNum='N916EV', ActualElapsedTime=78,\n",
    "                             CRSElapsedTime=80, AirTime=58, ArrDelay=-6, DepDelay=-4,\n",
    "                             Origin='ROA', Dest='ATL', Distance=357, TaxiIn=9, TaxiOut=11,\n",
    "                             Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=None,\n",
    "                             WeatherDelay=None, NASDelay=None, SecurityDelay=None,\n",
    "                             LateAircraftDelay=None).asDict()\n",
    "\n",
    "assert third.asDict() == correctRow, \"the row was expected to be %s but it was %s\" % (correctRow, third.asDict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Flights\n",
    ">TailNum = Identitification Code of Airplane  \n",
    "Each appearance of a TailNum in Airtraffic dataframe represents 1 flight  \n",
    "CountFlight returns number of flight for each airplane  \n",
    "By returning the number of occurrences of TailNum in Airtraffic DataFrame\n",
    "\n",
    "TailNum|count\n",
    "-------:|-----\n",
    "N693BR| 1526|\n",
    "N646BR| 1505|\n",
    "N476HA| 1490|\n",
    "N485HA| 1441|\n",
    "N486HA| 1439|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CountFlight(df):\n",
    "    # YOUR CODE HERE\n",
    "    CountDesc = (df\n",
    "                 .groupby(\"TailNum\")\n",
    "                 .count()\n",
    "                 .sort(f.desc(\"count\")) \n",
    "                )\n",
    "    #groupby() aggregates dataframe to groups by TailNum\n",
    "    #count() counts elements in each group\n",
    "    #sort(f.desc(\"count\")) sorts by the count \n",
    "    \n",
    "    CountDescExNull = (CountDesc\n",
    "                       .filter(CountDesc.TailNum.isNotNull())\n",
    "                      )\n",
    "    #filter() will only include non-Null TailNum\n",
    "    \n",
    "    return CountDescExNull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|TailNum|count|\n",
      "+-------+-----+\n",
      "| N824AS|    2|\n",
      "| N873AS|    1|\n",
      "| N856AS|    1|\n",
      "| N916EV|    1|\n",
      "| N886AS|    1|\n",
      "| N881AS|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "data = loadBind(testFile,airTraffic)\n",
    "CountFlight(data).show(20)\n",
    "#flightCount(data)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''flightCount tests'''\n",
    "\n",
    "data = loadBind(testFile,airTraffic)\n",
    "        \n",
    "correct = [Row(TailNum='N824AS', count=2),\n",
    "           Row(TailNum='N856AS', count=1),\n",
    "           Row(TailNum='N886AS', count=1),\n",
    "           Row(TailNum='N916EV', count=1),\n",
    "           Row(TailNum='N873AS', count=1),\n",
    "           Row(TailNum='N881AS', count=1)]\n",
    "\n",
    "correctRows(CountFlight(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelled Flights\n",
    ">CancellationCode = 'D'  \n",
    "returns dataframe showing which flights were cancelled and their destination  \n",
    "\n",
    "FlightNum|Dest|\n",
    "----:|-------\n",
    "4285| DHN|\n",
    "4790| ATL|\n",
    "3631| LEX|\n",
    "3632| DFW|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SecurityCancel(df):\n",
    "    output= spark.sql(\"\"\"SELECT FlightNum, Dest from airtraffic WHERE CancellationCode='D';\"\"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|FlightNum|Dest|\n",
      "+---------+----+\n",
      "|     1642| LAS|\n",
      "|      585| MSP|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "data = loadBind(sampleFile,airTraffic)\n",
    "SecurityCancel(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cancelledDueToSecurity tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(FlightNum=4794, Dest='JFK'), Row(FlightNum=4794, Dest='ATL')]\n",
    "\n",
    "correctRows(SecurityCancel(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Delay\n",
    "MaxWeatherDelay :: Airtraffic_df -> delay_df  \n",
    "delay_df is a singular dataframe containing the longest weather delay  \n",
    "Between start of January and end of March\n",
    "\n",
    "|_c0|\n",
    "|-------:|\n",
    "|1148|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxWeatherDelay(df):\n",
    "    output = spark.sql(\"\"\" SELECT MAX(WeatherDelay) as _c0 from airtraffic WHERE Month >= 1 and Month <= 3\"\"\")\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|_c0|\n",
      "+---+\n",
      "| 40|\n",
      "+---+\n",
      "\n",
      "longest weather delay between jan and mar is 40\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "data = loadBind(sampleFile,airTraffic)\n",
    "MaxWeatherDelay(data).show()\n",
    "\n",
    "delay = longestWeatherDelay(data).first()[0]\n",
    "print(f\"longest weather delay between jan and mar is {delay}\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''longestWeatherDelay tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = longestWeatherDelay(data).first()[0]\n",
    "\n",
    "assert test == 7, \"the longest weather delay was expected to be 7 but it was %s\" % test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Flights\n",
    ">NoFlight_df is a dataframe that contains the name of airlines that did not fly.  \n",
    "carriers_df has relation (Description,Code)  \n",
    "airtraffic_df has Code in it's schema for all planes that flew  \n",
    "\n",
    ">Goal. Get subsets of carriers_df that did not fly  \n",
    "\n",
    ">every element of airtraffic_df flew     \n",
    "carriers_df - (carriers_df intersect airtraffic_df) = planes that did not fly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def didNotFly(df):\n",
    "    #a = spark.sql(\"SELECT DESCRIPTION from AirTraffic INNER JOIN carriers on carriers.Code = AirTraffic.UniqueCarrier;\")\n",
    "    a = spark.sql(\"SELECT Description FROM carriers WHERE Code NOT IN (SELECT Code from airtraffic INNER JOIN carriers on carriers.Code = airtraffic.UniqueCarrier);\")\n",
    "    #a = spark.sql(\"Select Code FROM carrierse\")\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|  Tradewind Aviation|\n",
      "|     Comlux Aviation|\n",
      "|Master Top Linhas...|\n",
      "| Flair Airlines Ltd.|\n",
      "|           Swift Air|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = loadBind(testFile,airTraffic)\n",
    "didNotFly(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''didNotFly tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = didNotFly(data).count()\n",
    "\n",
    "assert test == 1489, \"the amount of airlines that didn't fly was expected to be 1489 but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights Vegas to JFK\n",
    "Return description and number of flights from Vegas to JFK  \n",
    "Countflight_df Schema (Description,Count)  \n",
    "carriers_df Schema (Code)\n",
    "\n",
    "|         Description|Num|\n",
    "|--------------------|---|\n",
    "|     JetBlue Airways|566|\n",
    "|Delta Air Lines Inc.|441|\n",
    "|US Airways Inc. (...|344|\n",
    "|American Airlines...|121|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flightsFromVegasToJFK(df):\n",
    "    fdata = CountFlight(df)\n",
    "    #loadDataAndRegister(\"flightCount\")\n",
    "    fdata.createOrReplaceTempView(\"flightCount\")\n",
    "    output = spark.sql(\"\"\"\n",
    "    SELECT Description, count as Num FROM \n",
    "        flightCount \n",
    "        INNER JOIN\n",
    "        (SELECT * FROM \n",
    "            carriers \n",
    "            INNER JOIN \n",
    "            (SELECT DISTINCT UniqueCarrier,TailNum FROM airtraffic WHERE Origin = 'LAS' AND Dest = 'JFK') as T1\n",
    "            ON\n",
    "            carriers.Code = T1.UniqueCarrier) as T2\n",
    "        ON flightCount.TailNum = T2.TailNum\n",
    "            ORDER BY Num DESC\n",
    "    \n",
    "    \"\"\")\n",
    "    # YOUR CODE HERE\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|         Description|Num|\n",
      "+--------------------+---+\n",
      "|       Titan Airways|  1|\n",
      "|Atlantic Southeas...|  1|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "data = loadBind(testFile,airTraffic)\n",
    "flightsFromVegasToJFK(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''flightsFromVegasToJFK tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(Description='Titan Airways', Num=1),\n",
    "           Row(Description='Atlantic Southeast Airlines', Num=1)]\n",
    "correctRows(flightsFromVegasToJFK(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Time\n",
    "TaxiTime : Returns dataframe of average time in taxi at each airport  \n",
    "Airtraffic relevant Schema(Origin, Dest, TaxiIn, TaxiOut)  \n",
    "Origin = Airport of origin  \n",
    "Dest = Airport of Dest  \n",
    "TaxiIn = Time spent in taxi at Origin  \n",
    "TaxiOut = Time spent in taxi at Dest  \n",
    "\n",
    "\n",
    "\n",
    "|airport|             taxi|\n",
    "|-------|-----------------|\n",
    "|    DLG|              4.0|\n",
    "|    BRW|5.051010191310567|\n",
    "|    OME|6.034800675790983|\n",
    "|    AKN|             6.75|\n",
    "|    SCC|6.842553191489362|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TaxiTime(df):\n",
    "    output = spark.sql(\"\"\"\n",
    "        SELECT T1.Origin as airport,(A1+A2)/2 as taxi FROM\n",
    "            (SELECT Origin,AVG(TaxiIn) AS A1 FROM AirTraffic GROUP BY Origin) AS T1\n",
    "            INNER JOIN\n",
    "            (SELECT Dest,AVG(TaxiOut) AS A2 FROM AirTraffic GROUP BY Dest) AS T2\n",
    "            ON\n",
    "                T1.Origin = T2.Dest\n",
    "            ORDER BY\n",
    "                taxi ASC\n",
    "            \n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|airport| taxi|\n",
      "+-------+-----+\n",
      "|    LAS| 11.0|\n",
      "|    JFK|13.25|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = loadBind(testFile,airTraffic)\n",
    "TaxiTime(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''timeSpentTaxiing tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(airport='LAS', taxi=11.0), Row(airport='JFK', taxi=13.25)]\n",
    "correctRows(TaxiTime(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Distance\n",
    "`distanceMedian` returns a DataFrame containing the median travel distance.\n",
    "\n",
    "|_ c0|\n",
    "|---|\n",
    "|583.0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceMedian(df):\n",
    "    a = spark.sql(\"\"\"\n",
    "        SELECT percentile(T1,0.5)as `_ c0` FROM\n",
    "            (SELECT Distance FROM AirTraffic) AS Tab(T1)\n",
    "    \"\"\")\n",
    "    # YOUR CODE HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| _ c0|\n",
      "+-----+\n",
      "|357.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = loadBind(testFile,airTraffic)\n",
    "distanceMedian(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''distanceMedian tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = distanceMedian(data).first()[0]\n",
    "assert test == 357.0, \"the distance median was expected to be 357.0 but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95th Percentile\n",
    "DataFrame containing the 95th percentile of carrier delay. \n",
    "\n",
    "Example output:\n",
    "\n",
    "|_ c0|\n",
    "|----|\n",
    "|77.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score95(df):\n",
    "    a = spark.sql(\"\"\"\n",
    "        SELECT percentile(T1,0.95)as `_ c0` FROM\n",
    "            (SELECT CarrierDelay FROM AirTraffic) AS Tab(T1)\n",
    "    \"\"\")\n",
    "    # YOUR CODE HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|_ c0|\n",
      "+----+\n",
      "|17.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = loadBind(testFile,airTraffic)\n",
    "score95(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''score95 tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = score95(data).first()[0]\n",
    "assert test == 17.0, \"the score95 was expected to be 17.0 but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelled Flights\n",
    "`cancelledFlights` finds airports where flights were cancelled. \n",
    "\n",
    "return: DataFrame containing columns \"airport\", \"city\" and \"percentage\".  \n",
    "airports relevant schema (airport,city)  \n",
    "percentage = (# cancelled flight/total flight)  \n",
    "\n",
    "Example output:\n",
    "\n",
    "|             airport|       city|         percentage|\n",
    "|--------------------|-----------|-------------------|\n",
    "|Pellston Regional...|   Pellston| 0.3157894736842105|\n",
    "|  Waterloo Municipal|   Waterloo|               0.25|\n",
    "|  Telluride Regional|  Telluride|0.21084337349397592|\n",
    "|Houghton County M...|    Hancock|0.19834710743801653|\n",
    "|Rhinelander-Oneid...|Rhinelander|            0.15625|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancelledFlights(df):\n",
    "    a = spark.sql(\"\"\"\n",
    "        SELECT airports.airport, airports.city, T1.percentage \n",
    "        FROM\n",
    "            (SELECT Origin,SUM(Cancelled)/COUNT(Origin) as percentage FROM AirTraffic \n",
    "            GROUP BY Origin) AS T1\n",
    "        INNER JOIN\n",
    "            airports\n",
    "        ON\n",
    "            T1.Origin = airports.iata\n",
    "        ORDER BY\n",
    "            T1.percentage DESC,\n",
    "            airports.airport DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|             airport|     city|percentage|\n",
      "+--------------------+---------+----------+\n",
      "|McCarran Internat...|Las Vegas|       0.5|\n",
      "|Roanoke Regional/...|  Roanoke|      0.25|\n",
      "| John F Kennedy Intl| New York|       0.0|\n",
      "+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = loadBind(testFile,airTraffic)\n",
    "cancelledFlights(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cancelledFlights tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(airport='McCarran International', city='Las Vegas', percentage=0.5),\n",
    "           Row(airport='Roanoke Regional/ Woodrum ', city='Roanoke', percentage=0.25)]\n",
    "correctRows(cancelledFlights(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares\n",
    "`leastSquares` calculates the linear least squares approximation for relationship between DepDelay and WeatherDelay.  \n",
    "y = Bx + c  where B is the slope  \n",
    "WeatherDelay = B*(DepDelay) + c  \n",
    "\n",
    "returns (y-intercept,B)\n",
    "\n",
    "filter out DepDelay < 0. If multiple WeatherDelay then average them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leastSquares(df):\n",
    "    try:\n",
    "        a = spark.sql(\"\"\"\n",
    "            SELECT DD, WD, DD*WD as Prod, DD*DD as Sqx  \n",
    "            FROM\n",
    "            (SELECT DepDelay as DD,AVG(WeatherDelay) as WD FROM AirTraffic\n",
    "            WHERE DepDelay > 0\n",
    "            GROUP BY DepDelay) AS T1\n",
    "        \"\"\")\n",
    "        # YOUR CODE HERE\n",
    "        xsum = a.groupBy().sum('DD').collect()[0][0]\n",
    "        ysum = a.groupBy().sum('WD').collect()[0][0]\n",
    "        psum = a.groupBy().sum('Prod').collect()[0][0]\n",
    "        sqxsum = a.groupBy().sum('Sqx').collect()[0][0]\n",
    "        #print(xsum)\n",
    "        N = a.count()\n",
    "        varM = ((N * psum)-(xsum*ysum))/((N * sqxsum)-(xsum**2))\n",
    "        varB = ((ysum*sqxsum)-(xsum*psum))/((N*sqxsum)-(xsum**2)) \n",
    "        return varB,varM\n",
    "    except error:\n",
    "        return 0.0,0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952.0, -56.0)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadBind(testFile,airTraffic)\n",
    "leastSquares(data)\n",
    "#leastSquares(data)[0].show(4)\n",
    "#print(leastSquares(data)[1])\n",
    "#print(leastSquares(data)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''leastSquaresTests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = leastSquares(data)\n",
    "assert test == (952.0, -56.0), \"the answer was expected to be (952.0, -56.0) but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
